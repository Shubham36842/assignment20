{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f9f757d9-f90c-438e-913b-bf7c61bd0aef",
   "metadata": {},
   "source": [
    "Q1. What is anomaly detection and what is its purpose?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73e2be4e-f6f8-4f16-829f-14ab213bfde4",
   "metadata": {},
   "source": [
    "Ans:Anomly detection is a technique used to detect oultliers in a dataset.Anomaly detection is a technique used in data analysis and machine learning to identify unusual or abnormal patterns or observations within a dataset. The purpose of anomaly detection is to distinguish data points that deviate significantly from the expected or typical behavior of the system or dataset. \n",
    "The main goals and purposes of anomaly detection are as follows:\n",
    "1.Identify Suspicious Events\n",
    "2.Quality Assurance\n",
    "3.Cybersecurity\n",
    "4.Healthcare\n",
    "5.inancial Fraud Detection"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20436c30-1eb9-45dc-bd9b-3f28f69696a5",
   "metadata": {},
   "source": [
    "Q2. What are the key challenges in anomaly detection?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ce46af3-bb53-40d0-bfab-5773dffe7831",
   "metadata": {},
   "source": [
    "Ans:Some of the key challenges in anomaly detection include:\n",
    "\n",
    "1.Imbalanced Data: In many real-world scenarios, anomalies are rare compared to normal instances. This class imbalance can make it challenging for anomaly detection algorithms to learn and accurately identify anomalies without being overwhelmed by the majority class.\n",
    "\n",
    "2.Labeling Anomalies: Obtaining labeled data for anomalies can be difficult and expensive. In many cases, anomalies are not well-documented or occur infrequently, making it hard to create a representative labeled dataset for training and evaluation.\n",
    "\n",
    "3.Feature Engineering: Selecting and engineering relevant features that capture the characteristics of both normal and anomalous data points is critical. In some cases, identifying informative features can be challenging, and the choice of features can greatly impact the performance of anomaly detection models.\n",
    "\n",
    "4.Algorithm Selection: Choosing the right anomaly detection algorithm for a specific problem is not always straightforward. Different algorithms have different strengths and weaknesses, and the choice depends on the nature of the data and the type of anomalies one is trying to detect.\n",
    "\n",
    "5.Adaptation to Data Changes: Anomaly detection models are often trained on historical data. When the data distribution changes over time due to evolving conditions or new anomalies, these models may become less effective. Ensuring that the models can adapt to changing data is a significant challenge."
   ]
  },
  {
   "cell_type": "raw",
   "id": "730bcf77-e252-4132-bd2d-bd52ba8f7840",
   "metadata": {},
   "source": [
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "699a5edf-d134-4271-97c0-1f5f054b6322",
   "metadata": {},
   "source": [
    "Ans:The key difference between unsupervised and supervised anomaly detection is the use of labeled data during training. Unsupervised methods are more flexible and applicable in scenarios where labeled anomaly data is unavailable or costly to obtain, but they may have higher false positive rates. Supervised methods, on the other hand, require labeled data but can offer higher precision and may perform better when dealing with known types of anomalies. The choice between the two approaches depends on the availability of labeled data and the specific requirements of the anomaly detection task."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b80f2e3-eb3d-4da4-9b9c-c9cc04802754",
   "metadata": {},
   "source": [
    "Q4. What are the main categories of anomaly detection algorithms?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10f683e7-08ff-45b8-8762-a8bfc14d23c6",
   "metadata": {},
   "source": [
    "Ans: The main categories of anomaly detection algorithms:\n",
    "1.Statistical Methods:\n",
    "(a)Z-Score/Standard Score: This method calculates the z-score of data points based on the mean and standard deviation of the dataset. Data points with high z-scores are considered anomalies.\n",
    "(b)Percentile-Based Methods: These methods identify anomalies by comparing data points to specific percentiles of the data distribution, such as the 99th percentile.\n",
    "\n",
    "2.Distance-Based Methods:\n",
    "(a)k-Nearest Neighbors (k-NN): k-NN identifies anomalies by measuring the distance between data points and their k-nearest neighbors. Data points with distant neighbors are potential anomalies.\n",
    "(b)Local Outlier Factor (LOF): LOF calculates the density of data points relative to their neighbors, identifying points with significantly lower density as anomalies.\n",
    "\n",
    "3.Density-Based Methods:\n",
    "(a)Gaussian Mixture Models (GMM): GMM models the data distribution as a mixture of Gaussian distributions and identifies anomalies as data points with low likelihood under the model.\n",
    "(b)DBSCAN (Density-Based Spatial Clustering of Applications with Noise): DBSCAN groups data points into clusters and identifies data points not belonging to any cluster as anomalies.\n",
    "\n",
    "4.Clustering Methods:\n",
    "(a)K-Means Clustering: K-means can be used for anomaly detection by considering data points that are distant from cluster centroids as anomalies.\n",
    "(b)Hierarchical Clustering: Hierarchical clustering can identify anomalies based on the structure of the hierarchical tree.\n",
    "\n",
    "5.Time-Series Anomaly Detection:\n",
    "(a)Moving Averages: Simple moving averages and exponentially weighted moving averages can be used to detect anomalies in time series data by identifying deviations from the expected trends.\n",
    "(b)Seasonal Decomposition: Decomposing time series data into seasonal, trend, and residual components allows for the detection of anomalies in the residual component."
   ]
  },
  {
   "cell_type": "raw",
   "id": "199501c8-7708-4f3d-a518-febf4e9dcdc6",
   "metadata": {},
   "source": [
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52a9dc56-2aba-447f-b5b2-0ecf367e594b",
   "metadata": {},
   "source": [
    "Ans:he main assumptions made by distance-based anomaly detection methods include:\n",
    "\n",
    "1.Assumption of Normality:\n",
    "Distance-based methods often assume that the majority of data points in the dataset represent normal behavior and that normal data points are distributed according to a particular statistical distribution, such as a Gaussian (normal) distribution. Anomalies are then considered deviations from this expected distribution.\n",
    "\n",
    "2.Local Density Assumption:\n",
    "Many distance-based methods assume that normal data points tend to cluster together and have relatively high local densities, while anomalies are isolated and have lower local densities. This assumption allows the algorithm to identify anomalies as data points with significantly lower local density compared to their neighbors.\n",
    "\n",
    "3.Distance Metric Assumption:\n",
    "These methods typically rely on a distance or dissimilarity metric, such as Euclidean distance, Mahalanobis distance, or other distance measures. The choice of distance metric can impact the algorithm's performance, and the assumption is that the chosen metric effectively captures the similarity or dissimilarity between data points.\n",
    "\n",
    "4.k-Nearest Neighbors (k-NN) Assumption:\n",
    "In k-NN-based methods, an assumption is made that normal data points have similar neighbors, whereas anomalies have dissimilar neighbors. By measuring the distances to the k-nearest neighbors of each data point, anomalies can be identified based on their distance from their neighbors.\n",
    "\n",
    "5.Threshold Assumption:\n",
    "Distance-based methods often use a threshold value to distinguish normal data points from anomalies. The assumption is that data points with distances exceeding the threshold are considered anomalies, while those below the threshold are normal. Setting an appropriate threshold is crucial and may require domain knowledge or tuning."
   ]
  },
  {
   "cell_type": "raw",
   "id": "053efde4-1f6f-48b8-9c19-af08f7d236a5",
   "metadata": {},
   "source": [
    "Q6. How does the LOF algorithm compute anomaly scores?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b4f3689-0ebc-4735-afed-9212d40f56e0",
   "metadata": {},
   "source": [
    "Ans:\n",
    "LOF computes anomaly scores:\n",
    "\n",
    "1.Neighborhood Definition: For each data point in the dataset, LOF defines a neighborhood based on a user-specified parameter, typically denoted as \"k.\" The parameter \"k\" represents the number of nearest neighbors to consider when assessing local density. LOF also considers a distance metric, such as Euclidean distance, to measure the distances between data points.\n",
    "\n",
    "2.Local Reachability Density (LRD): For each data point, LOF calculates its local reachability density (LRD). LRD quantifies the local density of a data point relative to its \"k\" nearest neighbors. It is computed as follows:\n",
    "\n",
    "For each data point \"A,\" find its \"k\" nearest neighbors.\n",
    "Compute the reachability distance between \"A\" and each of its neighbors. The reachability distance between \"A\" and a neighbor \"B\" is the maximum of the Euclidean distance between \"A\" and \"B\" and the k-distance of \"B,\" which represents the distance to the \"k\"-th nearest neighbor of \"B.\"\n",
    "Calculate the inverse of the average reachability distance as the LRD for data point \"A.\"\n",
    "\n",
    "3.Local Outlier Factor (LOF) Calculation: After calculating LRD for all data points, LOF computes the LOF for each data point. The LOF of a data point \"A\" measures how much its LRD deviates from the LRD of its \"k\" nearest neighbors. It is calculated as follows:\n",
    "\n",
    "For each data point \"A,\" find its \"k\" nearest neighbors.\n",
    "Compute the LRD of \"A.\"\n",
    "Compute the LRD of each neighbor.\n",
    "Calculate the LOF of \"A\" as the average ratio of the LRD of \"A\" to the LRDs of its neighbors.\n",
    "\n",
    "4.Anomaly Score: The LOF algorithm assigns an anomaly score to each data point based on its LOF value. A higher LOF indicates that a data point's local density is significantly lower than that of its neighbors, suggesting it is more likely to be an anomaly.\n",
    "\n",
    "5.Thresholding: Anomalies are identified based on a user-defined threshold for the LOF scores. Data points with LOF scores exceeding the threshold are considered anomalies."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d04f399c-c7bc-4e82-b502-3d1293a5714d",
   "metadata": {},
   "source": [
    "Q7. What are the key parameters of the Isolation Forest algorithm?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "437d644a-7b4e-4be5-b4ae-dc6c391efe63",
   "metadata": {},
   "source": [
    "Ans:The primary parameters of the Isolation Forest algorithm include:\n",
    "\n",
    "1.n_estimators (default = 100): This parameter determines the number of decision trees to be built in the forest. A larger number of trees can improve the algorithm's accuracy but may also increase computation time.\n",
    "\n",
    "2.max_samples (default = \"auto\"): It specifies the number of samples to be drawn from the dataset to build each tree in the forest. The default value of \"auto\" sets it to the minimum of 256 and the number of data points in the dataset. You can also specify an integer or a fraction of the total dataset size.\n",
    "\n",
    "3.contamination (default = \"auto\"): This parameter represents the expected proportion of anomalies in the dataset. If set to \"auto,\" it is estimated as 0.1 (10% of the dataset). You can also specify a float value between 0 and 1 to set the contamination level manually.\n",
    "\n",
    "4.max_features (default = 1.0): It determines the maximum number of features to consider when splitting a node in the decision tree. Setting it to 1.0 means that all features are considered, while values less than 1.0 mean that only a fraction of features is considered. Reducing the number of features can help with computational efficiency.\n",
    "\n",
    "5.bootstrap (default = False): If set to True, each tree in the forest is built using a bootstrapped sample (sampling with replacement) of the data. If set to False, no bootstrapping is performed."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2625b1fa-94fd-4624-9d48-520efb4b2f31",
   "metadata": {},
   "source": [
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93f3ca01-d1e4-42f9-b70a-727500182524",
   "metadata": {},
   "source": [
    "Ans:You can calculate the anomaly score as the ratio of the number of neighbors with a different class to the total number of neighbors (K). In this case, the anomaly score would be:\n",
    "\n",
    "Anomaly Score = Number of Neighbors with Different Class / K\n",
    "\n",
    "Anomaly Score = 8 / 10\n",
    "\n",
    "Anomaly Score = 0.8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e4b29fe-d842-4658-a441-1c463fc92b05",
   "metadata": {},
   "source": [
    "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69af7471-016e-4a65-8171-29a5a169ba1d",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
